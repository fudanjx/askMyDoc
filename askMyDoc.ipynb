{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index\n",
    "# !pip install langchain\n",
    "# !pip install openai\n",
    "# !pip install PyPDF2\n",
    "# !pip install python-pptx\n",
    "# !pip install docx2txt \n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ImageDocument(text='<s_menu><s_nm> REASONS WHY</s_nm><s_unitprice> 66% 50%</s_unitprice><s_cnt> 31%</s_cnt><s_price> 66%</s_price><sep/><s_nm> BETER INFORMED CUSTOMERS MAKE THE MOST ORYOUR PRODUCTOR SERVICE</s_nm><s_unitprice> 56 87</s_unitprice><s_cnt> 32%</s_cnt><s_price> 66%</s_price></s_menu><s_sub_total><s_subtotal_price> 66%</s_subtotal_price></s_sub_total><s_total><s_total_price> 66%</s_total_price><s_emoneyprice> Orchestra</s_emoneyprice></s_total>', doc_id='1d368ed7-c119-49b7-8361-caddfb286141', embedding=None, doc_hash='fe77dcd0b61bc5911ab3c48d2da7cbfb90fde91f0e6ebc9474ad7c8d0a1e0695', extra_info=None, image=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 196 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    ResponseSynthesizer,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./repo').load_data()\n",
    "print(documents)\n",
    "\n",
    "def open_file(filepath):\n",
    "    with open(filepath, 'r') as infile:\n",
    "        return infile.read()\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(user_input)\n",
    "        # response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n",
    "\n",
    "openai.api_key = open_file('./data/openaiapikey.txt')          \n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 6 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 273 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  what is this infographic about?\n",
      "Bot: \n",
      " This infographic appears to be about the reasons why customers should be better informed\n",
      "about a product or service, and the associated costs. It includes the unit price, count,\n",
      "and total price of the product or service.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 259 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  tell me more about the customer training\n",
      "Bot: \n",
      " It is not possible to answer this question with the given context information. The\n",
      "context information provided does not contain any information about customer training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 6 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 308 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  why customer training is important?\n",
      "Bot: \n",
      " Customer training is important because it helps customers become better informed about a\n",
      "product or service, which can lead to more effective and efficient use of the product or\n",
      "service. This can result in increased customer satisfaction, loyalty, and ultimately more\n",
      "sales. Additionally, customer training can help to reduce customer support costs by\n",
      "providing customers with the knowledge and skills to troubleshoot and resolve issues on\n",
      "their own.\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import textwrap  # Import the textwrap module\n",
    "\n",
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(openai.api_key, index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    # print(f\"Bot: {response['content']}\")\n",
    "    # Wrap the text and print the wrapped content\n",
    "    wrapped_content = textwrap.wrap(response['content'], width=90)\n",
    "    print(\"User: \", user_input)\n",
    "    print(\"Bot: \",)\n",
    "    for line in wrapped_content:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vendolly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
